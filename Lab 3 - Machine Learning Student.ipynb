{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Machine learning - Student notebook\n   \n   - Dataset, RDD APIs\n   - A dataframe is nothing but a table with columns, rows and headers\n   - In this notebook, we will work through a dataset to demostrate spark's SQL-like abilities. We will also look at its machine learning capabilities in context of this dataset.\n    \n### Dataset\n- The chosen dataset is that of Breast Cancer. This set is collected from digitized image of a fine needle aspirate (FNA) of a breast mass. For more information about this dataset, please visit https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n- Each row contains information about a single breast mass along with a diagnosis of malignant/benign for this mass\n    \n### Problem statement\n- We need to learn patterns describing malignant and benign masses and need to be able to place any future samples in either of these buckets.  \n- Thus, the problem is a classification one where given a set of probable results, we need to pick one result with confidance. \n    \n### Solution\n   We will go through the following steps:\n   - Read data (Spark SQL)\n   - Feature Engineering (Accumulators, Broadcasters, ml/mllib APIs)\n   - Data Visualizations (PixieDust, Seaborn)\n   - Modeling (Spark ML)\n   - Evaluation and prediction (Spark ML)\n   - Deployment (Watson ML repository)", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [], 
            "source": "from IPython.display import display\nfrom IPython.core.display import HTML ", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Requirement already satisfied: seaborn in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s280-a46e86a84e2274-95b1885704d4/.local/lib/python2.7/site-packages\r\n", 
                    "name": "stdout"
                }
            ], 
            "source": "!pip install seaborn", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Machine Learning process ###\n\n<center><img width=\"600px\" height=\"600px\" src=\"https://raw.githubusercontent.com/martinkearn/Content/master/Blogs/Images/MLProcess.PNG\"></center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Get dataset\n!wget https://raw.githubusercontent.com/joshishwetha/dsx-spark/master/data.csv", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Read dataset\n\n#### copy and paste the following code\n    data = spark.read.csv('data.csv',inferSchema='true',header='true')\n    data = data.drop('_c32')", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Data Exploration ###", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<center><img width=\"450px\" height=\"450px\" margin=\"auto\" src=\"https://img.clipartfest.com/f92b25f421c985eed2ccb12cdb4cbf54_vector-clip-art-cartoon-safari-kids-cartoon-clipart_800-557.jpeg\"></center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Basic operations on dataframe \n- To view contents: df.show(), df.take(n)\n- To transform columns: df.withColumn (\"column_name\",\"transformation\")\n- To rename columns: df.withColumnRenamed(\"old_name\",\"new_name\")", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Print first 3 rows of the input dataset ###\n\n    data.toPandas()[:3]", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Check datatypes\ndata.dtypes", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Make sure the data types are correct ####\n\n    from pyspark.sql.types import FloatType, StringType, IntegerType, DoubleType, ArrayType\n\n    for col in data.columns:\n        if col not in ['id','diagnosis']:\n            data = data.withColumn(col,data[col].cast(FloatType()))", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Replace null values with 0\n    data = data.na.fill(0)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### SQL\n\n#### Register dataframe as a temp table to query from (write sql on dataframes)\n    data.registerTempTable('cancer_data')", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Querying data\n- NOTE that the returned object is another Dataframe\n- One nice feature of the notebooks and python is that we can show it in a table via Pandas\n- Remember to perform an action to get your results (sql queries are also transformations :))", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "  \n    query = \"\"\"\n    select\n        diagnosis ,\n        count(1) as diagnosis_count\n    from cancer_data\n    group by diagnosis \n    \"\"\"\n    spark.sql(query).toPandas()\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Feature Engineering\n   - Dataset = Features + target\n   - Transforming features (categorical to numeric, continous to bins, scaling, normalization etc...)\n   - Selecting a subset of columns for wide datasets\n   - Exploding columns to make additional (synthetic features) for small datasets", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Feature set is numeric\n\n##### Step 1: Make sure your target is numeric\n- \"target\" is categorical (malignant/benign)\n- Our encoding: 1 = malignant(M), 0 = Benign(B) using sql like \"when\" statement", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Review your data\n#### Sample your data to view what it looks like before we make changes to the target column\n\n    data.select('diagnosis','radius_se').show(3)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Make label numeric\n\n##### copy and paste to run code\n    import  pyspark.sql.functions as F\n\n    data = data.withColumn('diagnosis',F.when(data.diagnosis=='M',1).otherwise(0))\n    data.toPandas().head(3)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Rename column from \"diagnosis\" to \"label\"\n\n    data = data.withColumnRenamed('diagnosis','label')", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "***********************************************************************************\n\n###   Detour  \n- Feature engineering is a major task and often involves complicated scripts\n\nWhat happens when\n   - Have custom script based on other datasets/static variables?\n   - I want to port over a current script to spark ?", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Accumulators, Broadcasters\n- One of the most basic things in scripts are variables (data and metadata). \n- Data vars = DataFrames/RDDs in spark\n- Metadata vars = Accumulators/broadcasters \n   \n- Metadata vars are not straightforward in the distributed world\n\n\n##### Accumulators\n   - Global variables which can be written into\n   \n##### Broadcasters\n   - Global variables to be read from \n************************************************************************************************", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Lets run a simple accumulator\n- Problem: Count the number of benign and malignant cases", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "outputs": [], 
            "source": "def count_labels(row):\n    global benign, malignant\n    if row.label==1:\n        malignant.add(1)\n    else:\n        benign.add(1)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### copy and run this code\n    benign = sc.accumulator(0)\n    malignant = sc.accumulator(0)\n\n    data.rdd.foreach(count_labels)\n\n    benign.value, malignant.value", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Broadcast the data variable so that each executor has its own version of it\n   - This way, it does need not be shipped to the executor with every call\n   - Saves network bandwidth", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Collect statistics for your dataset ###\n- Collect statistics for mean/std\n- store it in a variable called \"df\"\n\n\n        df = data.describe()\n        df.toPandas()", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Calculate correlations of each columns w.r.t the target\n- Change to do this in spark\n\n\n\n#### Select a subset of columns\n- Remove columns conveying very little information\n   - Zero variance columns\n   - Low correlation columns", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 3, 
            "outputs": [], 
            "source": "df = data.toPandas()\ncol_corr = sorted(df.corr()['label'].to_dict().items(),key=lambda x:x[1],reverse=True)\nfor col in col_corr[-5:]:\n    if col[0]!='id':\n        data = data.drop(col[0])", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "- Remove columns with low correlation, such columns contribute to very little information\n- Caution: Be wary of multicolinearity (\"leaky\" columns)", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Lets do some graphs!\n\n###  Here, we look at the columns and their correlations with the target variable.\n- We use PixieDust (charting library) for quick charts\n- Also takes RDDs as inputs, most of the other charting librabries take pandas dataframes as inputs\n\n\n##### copy and run code from here\n    import pixiedust\n    import pandas as pd\n    \n    df = pd.DataFrame(col_corr)\n    df = df.dropna()\n    df.columns = ['name','correlation']\n    viz_spark_df = sqlContext.createDataFrame(df)\n\n   \n    display(viz_spark_df)", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": false, 
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart", 
                        "keyFields": "name", 
                        "rowCount": "100", 
                        "aggregation": "SUM", 
                        "valueFields": "correlation"
                    }
                }
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Visualize correlations ####\n - Useful for datasets with small number of columns\n - A heatmap is useful to visualize how variables are related to each other (and not just the target)\n - Very useful while doing NLP applications to visualize similarity between documents (after using TFIDF)\n ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "outputs": [], 
            "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pyspark.mllib.stat import Statistics\n\nplt.style.use('ggplot')\nvalues = data.rdd.map(lambda x:list(x.asDict().values()))\ncorr_values = Statistics.corr(values)\nnames = data.rdd.map(lambda x:list(x.asDict().keys())).first()\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"darkgrid\")\nsns.heatmap(corr_values,xticklabels=names,yticklabels=names,square=True,vmin=0, vmax=1,\n                cmap=\"YlGnBu\")", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Vizualize existing classes", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### PCA  - Dimensionality Reduction \n - Very handy for visualization!", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 26, 
            "outputs": [], 
            "source": "###### PCA Visualize data ######\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.ml.feature import VectorAssembler \n\nfeature_cols = list(filter(lambda x:x not in ['id','label','dummy'],data.columns))\nassembler = VectorAssembler(inputCols=feature_cols,outputCol='features')\ndf = assembler.transform(data)\n\npca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\nmodel = pca.fit(df)\n\npca_result = model.transform(df)\nresult = pca_result.select(\"pcaFeatures\")\n", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### UserDefined Functions", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 9, 
            "outputs": [], 
            "source": "import regex\n\ndef g(point,co_ord):\n    if co_ord=='x':\n        value = regex.split('\\s+',point)[0][1:]\n    else:\n        value =  regex.split('\\s+',point)[1][:-1]\n    return value\n\nget_x_coord = UserDefinedFunction(lambda point:g(point,'x'), StringType())\nget_y_coord = UserDefinedFunction(lambda point:g(point,'y'), StringType())\nconvert_to_string = UserDefinedFunction(lambda point:str(point.toArray()),StringType())\n\npca_result = pca_result.withColumn('pcaFeatures_string',convert_to_string(pca_result['pcaFeatures']))\npca_result = pca_result.withColumn('x_coord',get_x_coord(pca_result['pcaFeatures_string']))\\\n                       .withColumn('y_coord',get_y_coord(pca_result['pcaFeatures_string']))\\\n                      \npca_result = pca_result.withColumn('x_coord',pca_result.x_coord.cast(FloatType()))\\\n                       .withColumn('y_coord',pca_result.y_coord.cast(FloatType()))\n    \nviz_df = pca_result.select('x_coord','y_coord','label').toPandas()\na = sns.lmplot(x='x_coord',y='y_coord',hue='label',data=viz_df,\n           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## We are all set to do some machine learning!\n\n<center><img width=\"350px\" height=\"350px\" margin=\"auto\" src=\"http://www.clipartkid.com/images/127/cartoon-explorer-characters-vectors-6cbSmI-clipart.jpg\"></center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Module layout\n##### It divides into two packages:\n- spark.mllib contains the original API built on top of RDDs.\n- spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n- Using spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n\n    \n    \n#### A common receipe\n- Make sure your data is numeric \n- Collect feature columns\n- Define transformations (Vector Indexer, Encoder)\n- Assemble columns into a single Vector column (VectorAssembler)\n- Split data into test and train\n- Define pipelines and estimators", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Prepare the dataset for machine learning\n    - Split data between test and train (70/30 split)\n    - Apply logistic regression and decision tree classifiers", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Convert 'label' to DoubleType()\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 35, 
            "outputs": [], 
            "source": "data = data.withColumn('label',data['label'].cast(DoubleType()))", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Split dataset into test and train sets (70/30) splits", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 36, 
            "outputs": [], 
            "source": "train, test = data.randomSplit([0.7,0.3])", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Count number of records in test and train sets", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 37, 
            "outputs": [
                {
                    "execution_count": 37, 
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(399, 170)"
                    }
                }
            ], 
            "source": "train.count(), test.count()", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Machine learning Pipelines\n- Easy to use API\n- Define stages to transform your dataset\n- Select any estimator (model) for prediction \n- Each pipeline will model a single estimator onto your model\n\n\n<center><img src=\"https://image.slidesharecdn.com/mlwithapachespark-2-160818115257/95/introduction-to-ml-with-apache-spark-mllib-47-638.jpg?cb=1490306278\" width=\"500px\" height=\"500px\"></center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 38, 
            "outputs": [], 
            "source": "from pyspark.ml.pipeline import Pipeline", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Define Assembler to combine feature into a single vetor (feature vector! :) )", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 39, 
            "outputs": [], 
            "source": "from pyspark.ml.linalg import Vectors #linear algebra package, has matrices, arrays, Vectors (dense and sparse)\nfrom pyspark.ml.feature import VectorAssembler \n\nfeature_cols = list(filter(lambda x:x not in ['id','label'],data.columns))\nassembler = VectorAssembler(inputCols=feature_cols,outputCol='features')", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "## Classification Pipelines \n- Logistic Regression\n- Tree based classifier\n\n- Classification models work towards making a Yes/No decision\n- Metrics to evaluate the model: Precision/Recall, AUC\n\n\n<center><img width=\"500px\" height=\"500px\" src=\"http://opexanalytics.com/cnt/uploads/2016/01/Red-Fish-High-Recall.jpg\"></center>", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 40, 
            "outputs": [], 
            "source": "#### Classification metrics ####\ndef calc_metrics(results):\n    metrics = {}\n    metrics['tp_0'] = results.filter((results.label==0)&(results.prediction==0)).count()\n    metrics['fn_0'] = results.filter((results.label==0)&(results.prediction==1)).count()\n    metrics['tn_0'] = results.filter((results.label==1)&(results.prediction==1)).count()\n    metrics['fp_0'] = results.filter((results.label==1)&(results.prediction==0)).count()\n    \n    metrics['tp_1'] = results.filter((results.label==1)&(results.prediction==1)).count()\n    metrics['fn_1'] = results.filter((results.label==1)&(results.prediction==0)).count()\n    metrics['tn_1'] = results.filter((results.label==0)&(results.prediction==0)).count()\n    metrics['fp_1'] = results.filter((results.label==0)&(results.prediction==1)).count()\n    \n    return metrics\n        \n### calc precision & recall ###\ndef precision_recall(results):\n    items = calc_metrics(results)\n    pre_0 = items['tp_0']/float((items['tp_0']+items['fp_0']))\n    pre_1 = items['tp_1']/float((items['tp_1']+items['fp_1']))\n    \n    recall_0 = items['tp_0']/float((items['tp_0']+items['fn_0']))\n    recall_1 = items['tp_1']/float((items['tp_1']+items['fn_1']))\n    \n    return {'pre_0':pre_0, 'recall_0':recall_0,'pre_1':pre_1,'recall_1':recall_1}", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "##### Logistic regression #####", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import LogisticRegression\n\n#define estimator and fit data\nestimator = LogisticRegression()\npipeline = Pipeline(stages=[assembler,estimator])\nlr_model = pipeline.fit(train)\n\n#get results\nresults = lr_model.transform(test)\nprecision_recall(results)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "outputs": [], 
            "source": "### Visualize results from logistic regression ###\npca_subset = pca_result.select('id','features','x_coord','y_coord')\nviz_df = pca_subset.join(results,on=['id','features']).select('x_coord','y_coord','prediction','label')\nviz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n\na = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)\na.set_titles(col_template=['Decision Tree results when label==\"Benign\"','Decision Tree results when label==\"Malignant\"'])", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "#### Tree classifier ####\n- Can you do the same using a decicion tree classifier?", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 6, 
            "outputs": [], 
            "source": "from pyspark.ml.classification import DecisionTreeClassifier\n# define estimator and fit data\nestimator = DecisionTreeClassifier()\npipeline = Pipeline(stages=[assembler,estimator])\ntree_model = pipeline.fit(train)\n\n# get results\nresults = tree_model.transform(test)\nprecision_recall(results)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Visualize the results of tree classifier", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": 7, 
            "outputs": [], 
            "source": "### Visualize the results of tree classifier ###\npca_subset = pca_result.select('ID','features','x_coord','y_coord')\nviz_df = pca_subset.join(results,on=['ID','features']).select('x_coord','y_coord','prediction','label')\nviz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n\na = sns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)", 
            "metadata": {
                "collapsed": false
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Clustering Pipeline\n- K-means clustering", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "outputs": [], 
            "source": "######## Kmeans clustering, join to the PCA set #########\nfrom numpy import array\nfrom math import sqrt\nfrom pyspark.ml.clustering import KMeans\n\nkmeans = KMeans(k=2,featuresCol='features')\nmodel = kmeans.fit(pca_result)\ncenters = model.clusterCenters()\nkmeans_result = model.transform(pca_result).select('id','features','prediction')\n\nviz_df = pca_result.join(kmeans_result,on=['id','features'],how='inner')\\\n                   .select('id','features','x_coord','y_coord','prediction','label')\n\nviz_df = viz_df.dropna(how='any',subset=['x_coord','y_coord']).toPandas()\n\nsns.lmplot(x='x_coord',y='y_coord',hue='prediction',col='label',data=viz_df,\n           fit_reg=False,palette={1:'red',0:'green'},size=6,aspect=1)", 
            "metadata": {
                "collapsed": false, 
                "scrolled": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Deploy to Cloud by using Watson Machine Learning Repo\n   Please follow demonstration! :D\n", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "outputs": [], 
            "source": "", 
            "metadata": {
                "collapsed": true
            }
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 0, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }, 
        "language_info": {
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }, 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python"
        }
    }
}